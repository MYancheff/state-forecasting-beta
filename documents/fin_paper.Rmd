---
title: "Modeling"
output: github_document
---


```{r load_libraries, echo=FALSE,warning=FALSE,message=FALSE,results='hide'}
knitr::opts_knit$set(root.dir = normalizePath("../"))
library(foreign)
library(tidyverse)
library(lubridate)
library(rpart)
library(ggthemes)
library(Amelia)
library(mice)
library(data.table)
```

```{r load_data,echo=FALSE,warning=FALSE,message=FALSE,results='hide'}
precinct_data_all = read_csv("data/Nevada_General_Election_Precinct_Level_Voting_Results_1984-2016.csv")
source("code/load_carls.R")
source("code/load_precinct_data.R")
source("code/load_econ_data.R")
```

```{r district_level_data, echo=FALSE}
# turn precinct data into data that identifies with a district in an eleciton
pres_summary = precinct_data_all %>% 
  filter(OFFICENAME %in% c("president","governor")) %>%
  filter(!is.na(VOTES)) %>%
  group_by(OFFICENAME,YEAR,STATE_LEGISLATIVE_CHAMBER,DIST_NAME,DISTRICT_NUM,PARTY_CODE) %>%
  summarise(vote_count = sum(VOTES),
            Party = get_pres_gov_party(PARTY_CODE[1])) %>%
  summarise(vote_dem = sum(ifelse(PARTY_CODE=="DEM",vote_count,0)),
            total_vote = sum(vote_count)) %>%
  ungroup() %>%
  gather(key=value_type,value=vote_value,vote_dem:total_vote)%>%
  unite(RaceValue,OFFICENAME,value_type) %>%
  spread(RaceValue,vote_value) %>%
  # deals with werid zeros by remoing them
  mutate(governor_vote_dem = ifelse(governor_vote_dem == 0,NA,governor_vote_dem))
  
# join the precinct and carl data which needs both name and number to uniquely identify
named_data = pres_summary %>% 
  mutate(DISTRICT_NUM = ifelse(is.na(DISTRICT_NUM),1,DISTRICT_NUM)) %>%
  filter()

joined_named_data = use_data %>%
  left_join(named_data,
            by=c("ELECTION_YEAR"="YEAR","Assembly"="STATE_LEGISLATIVE_CHAMBER","DISTRICT_NUM"="DISTRICT_NUM","DISTRICT_NAME"="DIST_NAME")) %>%
  filter(Assembly == "SENATE" & ELECTION_YEAR < 2012)

#join the precinct and carl data which needs only the number to identify (names do not necessarily match up correctly)
unnamed_data = pres_summary %>% 
  select(-DIST_NAME)

joined_unnamed_data = use_data %>%
  left_join(unnamed_data,
            by=c("ELECTION_YEAR"="YEAR","Assembly"="STATE_LEGISLATIVE_CHAMBER","DISTRICT_NUM"="DISTRICT_NUM")) %>%
  filter(!(Assembly == "SENATE" & ELECTION_YEAR < 2012))

#calculate variables we actually want to model
district_level_data = rbind(joined_named_data,joined_unnamed_data) %>%
  mutate(party_val = ifelse(wining_party == "DEM",1,
                            ifelse(wining_party == "REP",-1,0)),
         governor_perc_vote_dem = governor_vote_dem/governor_total_vote,
         president_perc_vote_dem =
           president_vote_dem/president_total_vote) %>%
  select(-(governor_total_vote:president_vote_dem))
```

```{r state_level_data,echo=FALSE,message=FALSE}
pres_party_in_power_data = read_csv("data-raw/state-legislative-data/pres_in_power.csv")

#calculate state level variables for each year
state_level_data = economic_data %>%
  left_join(pres_party_in_power_data,by=c("YEAR"="year")) %>%
  
  # spread party_in_power variable over all the years they are in power, not just every 4 years
  group_by((YEAR-1) %/% 4) %>%
  mutate(pres_party_in_power = paste(ifelse(is.na(pres_party_in_power),"",pres_party_in_power),collapse="")) %>%
  ungroup() %>%
  
  #calculate midterm and presidentail election penalties
  mutate(is_midterm = (YEAR %% 4) == 2,
         is_pres_election = (YEAR %% 4) == 0,
         pres_party_num = ifelse(pres_party_in_power=="DEM",1,
                                 ifelse(pres_party_in_power=="REP",-1,0)),
         midterm_penalty = pres_party_num * is_midterm,
         pres_elect_penalty = pres_party_num * is_pres_election) %>%
  
  select(YEAR:Unemployment_LOCAL,midterm_penalty,pres_elect_penalty) %>%
  mutate(lag_MHI_Change_LOCAL = lag(MHI_Change_LOCAL,order_by=YEAR))
```

```{r assembly_level_summary,echo=FALSE}
# gets legislature composition for a particular year and house 
year_summary = district_level_data %>%
  group_by(Assembly,ELECTION_YEAR) %>%
  summarise(assembly_composition = sum(party_val)/n()) %>%
  mutate(lag_assembly_composition = lag(assembly_composition,order_by=ELECTION_YEAR)) %>%
  ungroup()
```

```{r join_all,echo=FALSE}
#collect all districts into a single dataframe for modeling
joined_district_data = district_level_data %>%
  left_join(year_summary ,by=c("ELECTION_YEAR","Assembly")) %>%
  left_join(state_level_data ,by=c("ELECTION_YEAR"="YEAR")) %>%
  mutate(DIST_ID = factor(paste(DISTRICT_NAME,DISTRICT_NUM))) %>%
  select(-DISTRICT_NUM,-DISTRICT_NAME)
```


```{r lag_all,echo=FALSE}
lagged_district_data = joined_district_data %>%
  group_by(Assembly,DIST_ID) %>%
  #lagged by one year entry (i.e. 2 years for house, 4 years for senate)
  mutate(lag_governor_perc_vote_dem = lag(governor_perc_vote_dem,order_by=ELECTION_YEAR),
         lag_president_perc_vote_dem = lag(president_perc_vote_dem,order_by=ELECTION_YEAR),
         lag_assembly_vote_dem = lag(assembly_vote_dem,order_by=ELECTION_YEAR),
         lag_incumbent_factor = lag(incumbent_factor,order_by=ELECTION_YEAR)) %>%
  ungroup()
```

```{r impute_missing,echo=FALSE}
impute_data = function(all_data){
  mice_imputed_data = complete( mice( all_data ,printFlag=FALSE,m=1))
  
  
  # conducts amelia imputation
  #removed_amelia_cols = all_data %>%
  #  select(DIST_ID,Assembly,wining_party,party_val,pres_elect_penalty,has_incumbent,lag_has_incumbent)
  
  #amelia_data = all_data %>%
  #  select(-DIST_ID,-Assembly,-wining_party,-party_val,-pres_elect_penalty,-has_incumbent,-lag_has_incumbent)
  
  #amelia_fit <- amelia(amelia_data,
  #                     m=1,
  #                     p2s=0,#2 is verbose, 1 is not as verbose, 0 is no output
  #                     ts="ELECTION_YEAR")
  #amelia_imputed_data = cbind(amelia_fit$imputations[[1]],removed_amelia_cols)
  
  #check if mice_imputed_data or amelia_imputed_data works better
  
  mice_imputed_data
}
zero_off_election_years = function(data){
  data %>%
  mutate(governor_perc_vote_dem = 
           ifelse(ELECTION_YEAR %% 4 == 2,governor_perc_vote_dem,0),
         president_perc_vote_dem =
           ifelse(ELECTION_YEAR %% 4 == 0,president_perc_vote_dem,0),
         lag_governor_perc_vote_dem = ifelse(ELECTION_YEAR %% 4 == 0,lag_governor_perc_vote_dem,0),
         lag_president_perc_vote_dem = ifelse(ELECTION_YEAR %% 4 == 2,lag_president_perc_vote_dem,0))
}
```


```{r modeling,echo=FALSE}
place_has_incumbent = function(data){
  data %>%
    mutate(has_incumbent = ifelse(incumbent_factor==0,0,1),
           lag_has_incumbent = ifelse(lag_incumbent_factor==0,0,1))
}
get_model_accuracy = function(assembly_data,model_acc,testyear){
  train_data = assembly_data %>%
    filter(ELECTION_YEAR < testyear) %>%
    impute_data() %>%
    # needs to happen after imputation or else the imputation is terreble
    zero_off_election_years()
  
  test_data = assembly_data %>%
    impute_data() %>%
    filter(ELECTION_YEAR == testyear) %>%
    zero_off_election_years() 
  
  model_acc(train_data,test_data)
}
testyear = 2014
get_accuracies = function(assembly_data,generate_model_fn){
  years = assembly_data %>%
    select(ELECTION_YEAR) %>%
    distinct()
  estimate_years = years$ELECTION_YEAR[years$ELECTION_YEAR > 1996]
  accuracies = sapply(estimate_years,function(test_year){
    get_model_accuracy(assembly_data,generate_model_fn,test_year)
  })
  possible_elections = sapply(estimate_years,function(test_year){
    nrow(assembly_data %>%
      filter(ELECTION_YEAR == test_year))
  }) 
  data.frame(accuracies,estimate_years,possible_elections)
}
get_assembly_data = function(assembly){
  start_year = 1986
  assembly_data = lagged_district_data %>%
    filter(Assembly==assembly,
           ELECTION_YEAR>=start_year) %>%
    place_has_incumbent()
}
```

```{r model_accuracy_plotter,echo=FALSE}
get_both_model_accuracies_plot = function(model1,model2){
  assembly_data = get_assembly_data("HOUSE")
  
  probit_acccs = get_accuracies(assembly_data,probit_model_acc)
  linear_acccs = get_accuracies(assembly_data,linear_model_acc)
  
  act_data = year_summary %>%
    mutate(actual_data = (1+assembly_composition)/2,
           lag_actual_data = (1+lag_assembly_composition)/2) %>%
    filter(ELECTION_YEAR> 1996,
           Assembly=="HOUSE") %>%
    select(ELECTION_YEAR,actual_data,lag_actual_data)
  
  act_data$linear_fit = linear_acccs$accuracies
  act_data$logit_fit = probit_acccs$accuracies
  
  act_data %>%
    gather(key=fit_type,value=senate_comp_value,-ELECTION_YEAR,-actual_data) %>%
    mutate(senate_comp_value = abs(senate_comp_value - actual_data)) %>%
    ggplot(aes(x=ELECTION_YEAR,y=senate_comp_value,color=fit_type)) +
      geom_line() + 
  #    scale_y_continuous(limits = c(0,1.7)) +
      ggtitle("Prediction Model Errors") + 
      xlab("Election Year of Test") + 
      ylab("Proportion Democrat in State Assembly") + 
      scale_color_discrete(labels=c("Previous Year's value","Linear Statewide Model","Logit District Level Model"),
                        guide = guide_legend(title = "Party"))
}
```



```{r variance_calc,echo=FALSE,eval=FALSE}
#takes too long to actual run (~30 minutes)
num_samples = 8
all_accuracies_house = rbindlist(lapply(1:num_samples,function(n)get_accuracies(get_assembly_data("HOUSE"))))
all_accuracies_senate = rbindlist(lapply(1:num_samples,function(n)get_accuracies(get_assembly_data("SENATE"))))
```

```{r variance_plot,echo=FALSE,eval=FALSE}
all_accuracies_house$Assembly="HOUSE"
all_accuracies_senate$Assembly="SENATE"
all_accuracies = rbind(all_accuracies_house,all_accuracies_senate)
ggplot(all_accuracies,aes(x=estimate_years,y=accuracies/possible_elections,color=Assembly)) +
  geom_point() + 
  geom_smooth()
```



# Modeling State Legislative Elections In Nevada

Nevada is a politically volatile state, with the state legislature changing every couple of years recently.

```{r plot_leg_composition,echo=FALSE,warning=FALSE}
line_data = data.frame(YEAR=c(min(year_summary $ELECTION_YEAR),max(year_summary $ELECTION_YEAR)),party_composition=c(0.5,0.5))

year_summary %>%
  mutate(assembly_composition = assembly_composition*0.5 + 0.5,
         dem_years = assembly_composition,
         rep_years = 1-assembly_composition) %>%
  gather(dem_years,rep_years,key=party_year,value=party_composition) %>%
  ggplot(aes(x=ELECTION_YEAR,y=party_composition,fill=party_year)) + 
    geom_area(position="stack") + 
    facet_grid(~Assembly) + 
    geom_line(data=line_data,mapping=aes(x=YEAR,y=party_composition,fill=NA),color="black",size=1) + 
    theme_few() + 
    xlab("Year") + 
    ylab("Party control of assembly") + 
    ggtitle("Nevada State Legislature Party Composition over Time") + 
    scale_fill_manual(values=c("blue","red"),
                      labels=c("Democratic","Republican"),
                      guide = guide_legend(title = "Party"))
    
```

We want to model state legislative elections, so that we can predict future election results. In particular, the most important result people usually want to know is the party composition of the state legislatures, as this can determine the policy agenda for the state. 

### Modeling structure 

As we are trying to predict state legislative outcomes, any given model would look like this:

Percent of Legislature that are Democrat ~ Known variables before election

### Modeling problems

Unfortunately, forecasting state legislative elections is difficult. There is a lot of unobserved characteristics especially at the district level, with certain candidates being of better qualities than others, or particular features of the districts at hand. 

Unfortunately, in Nevada there has been a lot of redistricting, and a lack of precinct level maps that would allow us to account for this by performing a geographic join. Without those maps, we cannot account for unobserved variables in districts by tracking them over a long period of time. 

So we need to gather as much information about a district as possible. For example, one clearly useful variable is the extent to which the candidate of the Democratic party won in the prior year. Another one is whether that candidate was an incumbent or not, and whether they are running again in the upcoming election.

We also need to gather several variables at the state and national level, including economic variables such as GDP and median household income, several political variables such as the party of the president, and governor, and the state legislative composition in the previous year.


### Data Tidying and Joining

The format of the tidied dataset we want to model is this:

Every row is a (Year, Assembly, District) triple. This is a nice unit because it is the unit we want to run models on. However, none of our data is in this form. All of our data is in a (Year, Candidate) format. We have data both at the precint and the district level, with an identifier that can join them. So in each dataset, before joining, we need to summarize all the precincts in the district and then summarize the sort of covariates we want from that bit of candidate level data. See the Data wrangling section of the code below for more detail. 

### Lagging variables

Most of the useful information about the districts (e.g. voting results) cannot being predicted for the current election, and so most dependent variables must be taken from the previous election. In years other than those between redistricting periods, this is a very simple matter. In redistricting years, ideally we would have kept all the districts which stayed the same, and then try to derive the missing data using imputation (for practical reasons this was not done).

### Missing-Data Imputation

Much of our data was missing. However, ordinary models usually expect all variables to be present. So in order to fit a model, we need to fill in this data. This general process is called missing data imputation, or just imputation. In general, an imputation process takes in a dataframe with certain values as NA, and outputs a dataframe without any NA entries. 

One way to do this is to simple fill in NAs with the mean or median of all the entries in that collumn. However, with so much dependence between variables, this seriously reduced the quality of the model. I also tried to use Amelia II, via R's Amelia package. This imputation process is based on the multivariate normal distribution. It is fairly flexible, allowing you to specify categorical and numeric data. However, it was extremely fiddly, as it does not work with co-linear variables, and it also assumes that values are pulled from a multivariate normal distribution. I also could not get its categorical variable feature working (it gave a non-standard error). So I chose to use R's MICE (Multiple Imputation by Chained Equasions) package, which can work with a wide variety of data, and seems to work amazingly well out of the box. But perhaps we can measure how well it predicts the data more precisely (See Question 1).

Also note that the MICE imputation method is randomized, which creates fairly signficant variance in our prediction. Unfortunately, it is also slow enough that measuring this variance experimentally is impractical (a single run takes about a two minutes, so 20 runs will take 40).

### Modeling Design

Two of the simplest ways of predicting state legislative composition are these:

* A simple linear model. The outcome (percentage of Democrats in the legislature) is fit as a linear function of the known variables. 
* A logit generalized linear model. The outcome of each district is fit as a logit model of which party won. Then we can predict who will win at the district level by which party has over 50% probability of winning. Then we can calculate the number of districts it predicts democrats will win in order to get the overall election result. 

The linear model is nice because it . However, the number of outcomes is only the number of elections for which we have data for, which is not very many. That makes it extremely easy to overfit. 

The logit model has many more outcomes to train for, but district level outcomes have much more noise, and are generally harder to predict.


Because overfitting is such a serious potential problem, especially for the first model, we want as few variables as possible to model with. So before trying to implement this model, we should collect some data, and see if it seems to be useful, and how the variables might interact with others. 

## Quality of Covariates

Even before modeling, we can at least attempt to see which covariates are important. The one we are the most interested in are the governor and presidential election results for the previous year, since we spent so much time collecting that data. And we can see that it does have a strong relationship with district level outcomes, which is what we hoped for.

```{r gov_pres_district_correlation,echo=FALSE,fig.width=6,fig.height=20}
na_to_zero = function(val)ifelse(is.na(val),0,val)

lagged_district_data %>%
  mutate(elect_type= ifelse(ELECTION_YEAR %% 4 == 2,"President","Governor"),
         lag_vote_dem = na_to_zero(lag_governor_perc_vote_dem)+na_to_zero(lag_president_perc_vote_dem)) %>%
  filter(#abs(assembly_vote_dem) != 1,
         lag_vote_dem != 0) %>%
  ggplot(aes(y=lag_vote_dem,x=assembly_vote_dem/2+0.5))+
  geom_smooth(method="lm") + 
  geom_point() + 
  ggtitle("Predictive Power of President Vote Composition on State Legislative Vote at the District Level") + 
  ylab("Percent vote for Republican State Legislator") + 
  xlab("Percent vote for Republican President in previous election") + 
  facet_grid(ELECTION_YEAR~elect_type)
```

Even though there is clearly a strong correlation, it does not guarantee that it is useful. Theoretically, the lagged state legislature outcome could contain the same information about the district. 

However, it does provide some strong evidence. One of the interesting things about the above plot is that years after 2008 seem to have a stronger correltion between presidential and legislative elections than years before 2008. Perhaps this is the result of an increase in partisanship, but we will need to somehow capture this trend in our model, in order to capture the full benefit of this variable. Two potential ways that might be able to do this are:

1. Put a linear interaction between Lagged Percent Vote President and Year to capture the general increase over time
2. Make a non-linear interaction, such as a dummy varaible that marks elections after 2006. 

#### Incumbency

Another important varaible is incumbency. We know that incumbents tend to win far more than non-incumbents. But also we can measure how much they win by.

```{r incumbency_plot}
vote_ratio_data <- all_years_data %>%
  group_by(ELECTION_YEAR, Assembly, DISTRICT_NUM,DISTRICT_NAME) %>%
  summarise(perc_vote_WINNER = max(CANIDATE_VOTE_TOTAL)/sum(CANIDATE_VOTE_TOTAL),
            incumbent_present = sum(INCUMBENCY_DUMMY),
            incumbent_won = sum(INCUMBENCY_DUMMY * CANIDATE_VOTE_TOTAL) == max(CANIDATE_VOTE_TOTAL)) %>%
  filter(perc_vote_WINNER != 1.00) %>%
  mutate(incumbent_present = ifelse(incumbent_won,"Incumbent Won",
                                    ifelse(incumbent_present >= 1, "Incumbent Lost",
                                    "Open Seat Election")))

#Graph
g3 <- ggplot(vote_ratio_data, aes(perc_vote_WINNER)) +
  geom_density() +
  facet_wrap(~incumbent_present) + 
  xlab("Proportion Vote of Winner") + 
  ggtitle("Vote Captured by Winner in Contested Elections")
g3
```

The above plot suggests that incumbents usually win by far more than non-incumbents. But they also lose on occasion. What variables can help explain when they lose? In theory, as the electorate is more familiar with the incumbent than the non-incumbent, their vote will most likely depend on whether they think they are doing a good job, and less on factors concerning party affiliation, such as national elections etc. So perhaps we can interact the political variables with whether it is an open seat election or not.

The exploration above, and other's models brought me to the following models

```{r models_act}
linear_model_acc = function(train_data,test_data){
  model = lm(assembly_composition ~ 
       #state level data
       lag_assembly_composition*GDPperCapita_CHANGE_US + 
       midterm_penalty + 
       pres_elect_penalty + 
        
       # district level variables
       incumbent_factor + 
       lag_governor_perc_vote_dem + 
       lag_president_perc_vote_dem + 
       lag_assembly_vote_dem
       ,data=train_data)
  
  # caculate 
  guessed_outcome = predict(model,test_data,type = "response")
  prop_steats_rep = mean((guessed_outcome+1)/2)
  prop_steats_rep
}
probit_model_acc = function(train_data,test_data){
  model = glm((party_val+1)/2 ~
       #state level data
       lag_assembly_composition*GDPperCapita_CHANGE_US + 
       MHI_Change_LOCAL +
       lag_MHI_Change_LOCAL +
       GDPperCapita_CHANGE_US + 
       midterm_penalty + 
       pres_elect_penalty + 
        
       # district level variables
       incumbent_factor + 
       lag_incumbent_factor + 
       (lag_governor_perc_vote_dem + 
       lag_president_perc_vote_dem + 
       lag_assembly_vote_dem) * has_incumbent
       ,data=train_data
       ,family="binomial")
  guessed_outcome = predict(model,test_data,type = "response")
  discrete_guessed_outcome = ifelse(guessed_outcome>0.5,1,0)
  prop_steats_rep = sum(discrete_guessed_outcome)/nrow(test_data)
  prop_steats_rep
}
```

## Actual Predictive Results

### Train/Test Method

In order to make sure that the model actually works, I tested it on previous years, by running the following algorithm.

Given a particular test year, 

1. Eliminate future years from the dataset
2. Conduct imputation on filtered dataset
3. Train model on that imputed dataset
4. Predict outcome with that model
5. Compare that outcome with the actual outcome

Note that the imputation step has to occur after the future years are eliminated since in the test year, we could not have had that infromation to help that imputaton.

And then I ran that for several years prior to 2016. 

Here are some results from that method:

```{r model_data,echo=FALSE,cache=TRUE,warning=FALSE}
get_both_model_accuracies_plot(probit_model_acc,linear_model_acc)
```

The error is the distance between the actual and the predicted.

As you can see, while the models correctly guesses electoral compotition sometimes, it leaves a lot to be desired. The linear model behaves disasteriously sometimes, predicting results which are well beyond what is actually possible (the maximum value is 1). But it does much better after 2010. This is most likely due to significant overfitting, and it suggests reducing the number of variables may improve the results. The logit model does pretty well from 2004-2010, but it does terrebly after 2010. Unfortunately, neither model is better than the naive method of simply guessing that the next year is identical to the previous one. 

So how might we fix this?

Luckily, we can simply take any idea we want, such as those proposed in the Covariates section above, and try them. Then, generate a plot like the above, and see if it seems to do better than before. This puts us in the following workflow

1. Get the simplest model that might work well.
2. Guess a model you think might work better
3. Run the test-train method above on that model
4. Plot it, or calculate the mean squared error, to see if it does better.
5. If it does, then set it as your default model, if not, then throw it out.
6. Go back to step 2.


## Code

All of the code to generate the plots and model the data is below. 

### Set-up code
```{r ref.label='load_libraries',eval=FALSE}
```

The code to load some of the code is in other files. 

```{r ref.label='load_data',eval=FALSE}
```


### Data wrangling code
```{r ref.label='district_level_data',eval=FALSE}
```
```{r ref.label='state_level_data',eval=FALSE}
```
```{r ref.label='assembly_level_summary',eval=FALSE}
```
```{r ref.label='join_all',eval=FALSE}
```
```{r ref.label='lag_all',eval=FALSE}
```
### Imputation Code

```{r ref.label='impute_missing',eval=FALSE}
```

### Modeling Code

```{r ref.label='modeling',eval=FALSE}
```

### Model Error Ploting

```{r ref.label='model_accuracy_plotter',eval=FALSE}
```

### Plotting code

#### Legislative composition

```{r ref.label='plot_leg_composition',eval=FALSE}
```

#### President/Governor state outcomes correlation 

```{r ref.label='gov_pres_district_correlation',eval=FALSE}
```

#### Incumbency effect magnitude  

```{r ref.label='incumbency_plot',eval=FALSE}
```

#### Model test output 

```{r ref.label='model_data',eval=FALSE}
```

